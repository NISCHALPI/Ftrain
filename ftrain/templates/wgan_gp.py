"""Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) Lightning Module.

This module implements the WGAN-GP algorithm, which extends the standard GAN framework
with Wasserstein distance and gradient penalty for improved stability during training.

References:
- Wasserstein GAN-GP: https://arxiv.org/pdf/1704.00028.pdf

Args:
    generator (nn.Module): The generator network that produces fake samples.
    discriminator (nn.Module): The discriminator network that distinguishes real from fake samples.
    gp_weight (float, optional): Weight for the gradient penalty term. Defaults to 10.
    critic_iters (int, optional): Number of critic iterations per generator iteration. Defaults to 5.

Raises:
    AssertionError: If the generator doesn't have required attributes.
    ValueError: If latent dimension is not int or tuple/list/torch.Size.

Note:
    This module uses LayerNorm instead of BatchNorm to decorrelate samples.

"""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from pytorch_lightning.utilities.types import STEP_OUTPUT


import warnings

import pytorch_lightning as pl
import torch
import torch.nn as nn

__all__ = ["WGAN"]


class WGAN(pl.LightningModule):
    """Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) Lightning Module.

    This module implements the WGAN-GP algorithm, which extends the standard GAN framework
    with Wasserstein distance and gradient penalty for improved stability during training.

    Args:
        generator (nn.Module): The generator network that produces fake samples.
        discriminator (nn.Module): The discriminator network that distinguishes real from fake samples.
        gp_weight (float, optional): Weight for the gradient penalty term. Defaults to 10.
        critic_iters (int, optional): Number of critic iterations per generator iteration. Defaults to 5.

    Raises:
        AssertionError: If the generator doesn't have required attributes.
        ValueError: If latent dimension is not int or tuple/list/torch.Size.

    Note:
        This module uses LayerNorm instead of BatchNorm to decorrelate samples.

    """

    def __init__(
        self,
        generator: nn.Module,
        discriminator: nn.Module,
        gp_weight: float = 10,
        critic_iters: int = 5,
        *args,
        **kwargs,
    ) -> None:
        """Initialize the WGAN-GP module.

        Args:
            generator (nn.Module): The generator network.
            discriminator (nn.Module): The discriminator network.
            gp_weight (float): Weight for the gradient penalty term.
            critic_iters (int): Number of critic iterations per generator iteration.
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.

        Raises:
            AssertionError: If the generator is missing required attributes.
            ValueError: If latent dimension is not int or tuple/list/torch.Size.
        """
        super().__init__(*args, **kwargs)

        assert hasattr(
            generator, "latent_dim"
        ), "Generator must have a latent_dim attribute"

        assert hasattr(generator, "prior"), "Generator must have a prior attribute"

        assert isinstance(
            generator.prior, torch.distributions.Distribution
        ), "Generator prior must be a torch.distributions.Distribution"

        assert critic_iters > 0, "Critic iters must be greater than 0"
        self.critic_iter = critic_iters
        self.generator = generator
        self.discriminator = discriminator
        self.gp_weight = gp_weight

        # Warn if Batch Norm is used
        self._batch_norm_warn()

        # Eanble Manual Optimization
        self.automatic_optimization = False

        # Save Hyperparameters
        self.save_hyperparameters('gp_weight')

    def forward(self, z: torch.Tensor) -> torch.Tensor:
        """Forward pass for the WGAN-GP module.

        Args:
            z (torch.Tensor): Latent vectors to generate fake samples.

        Returns:
            torch.Tensor: Fake samples generated by the generator.
        """
        return self.generator(z)

    def _batch_norm_warn(self) -> None:
        """Check for BatchNorm layers and provide warnings.

        Raises:
            UserWarning: If BatchNorm layers are found in either the generator or discriminator.
        """
        # Batch Norm Warning
        for submodule in self.discriminator.modules():
            if isinstance(submodule, (nn.BatchNorm2d, nn.BatchNorm1d, nn.BatchNorm3d)):
                warnings.warn(
                    'Discriminator has BatchNorm layers. Consider using LayerNorm instead. Look at the WGAN-GP paper for more details.'
                )
                break

        for submodule in self.generator.modules():
            if isinstance(submodule, (nn.BatchNorm2d, nn.BatchNorm1d, nn.BatchNorm3d)):
                warnings.warn(
                    'Generator has BatchNorm layers. Consider using LayerNorm instead. Look at the WGAN-GP paper for more details.'
                )
                break

        pass

    def _resolve_batch(self, batch: torch.Tensor | tuple) -> torch.Tensor:
        """Resolve batch input to a torch.Tensor.

        Args:
            batch (torch.Tensor or tuple): Input batch.

        Returns:
            torch.Tensor: Resolved batch tensor.


        Raises:
            ValueError: If batch is not a torch.Tensor or tuple of torch.Tensor.
        """
        if isinstance(batch, torch.Tensor):
            return batch
        if isinstance(batch, (tuple, list)):
            return batch[0]
        raise ValueError("Batch must be a torch.Tensor or a tuple of torch.Tensor")

    def _sample_latent_from_prior(self, batch_size: int) -> torch.Tensor:
        """Sample latent vectors from the generator's prior distribution.

        Args:
            batch_size (int): Number of latent vectors to sample.

        Returns:
            torch.Tensor: Sampled latent vectors.
        """
        if isinstance(self.generator.latent_dim, int):
            return self.generator.prior.sample(  # type: ignore
                (batch_size, self.generator.latent_dim)
            ).to(self.device)

        if isinstance(self.generator.latent_dim, (tuple, list, torch.Size)):
            return self.generator.prior.sample(  # type: ignore
                (batch_size, *self.generator.latent_dim)
            ).to(self.device)

        raise ValueError("Latent dim must be int or tuple/list/torch.Size")

    def _wgan_discriminator_step(self, batch_x: torch.Tensor) -> torch.Tensor:
        """Perform a discriminator step using the WGAN-GP loss.

        Args:
            batch_x (torch.Tensor): Batch of real samples.

        Returns:
            torch.Tensor: Total loss for the discriminator step.
        """
        _, optim_D = self.optimizers()  # type: ignore[misc]

        # Zero out gradients
        self.discriminator.zero_grad()

        # Sample from prior
        z = self._sample_latent_from_prior(batch_x.shape[0])

        # sample eps from uniform distribution | different for each sample
        eps = torch.rand(  # type: ignore[call-overload]
            batch_x.shape[0],
            *[1 for _ in range(len(batch_x.shape) - 1)],
            device=self.device,
        )

        # Generate fake samples | Do not track gradients from generator
        with torch.no_grad():
            batch_fake = self.generator(z)

        # Calculate Discriminator output for real and fake samples
        wasserstein_loss = self.discriminator(batch_fake) - self.discriminator(batch_x)

        # Mixtures of real and fake samples
        mixtures = eps * batch_x + (1 - eps) * batch_fake
        mixtures.requires_grad = True  # Enable gradient tracking

        # Mixture forward pass
        output = self.discriminator(mixtures)

        # Calculate gradients of discriminator output w.r.t mixtures
        gradients = torch.autograd.grad(
            outputs=output,
            inputs=mixtures,
            grad_outputs=torch.ones_like(output, device=self.device),  # type: ignore[arg-type]
            create_graph=True,
            retain_graph=True,
            allow_unused=True,
        )[0]

        # Calculate total loss which is the sum of wasserstein loss and gradient penalty
        total_loss = (
            wasserstein_loss.mean()
            + self.gp_weight * (torch.norm(gradients, p='fro') - 1) ** 2
        )

        # Do Backpropagation Manually
        self.manual_backward(total_loss)

        # Update Discriminator
        optim_D.step()

        # Log losses
        self.log(
            "discriminator_loss", total_loss, on_step=True, on_epoch=True, prog_bar=True
        )

        return total_loss

    def _wgan_generator_step(self, batch_x: torch.Tensor) -> torch.Tensor:
        """Perform a generator step using the WGAN-GP loss.

        Args:
            batch_x (torch.Tensor): Batch of real samples.

        Returns:
            torch.Tensor: Generator loss for the step.
        """
        # Get generator optimizer
        optim_G, _ = self.optimizers()  # type: ignore[misc]

        # Zero out gradients
        self.generator.zero_grad()

        # Sample from prior
        z = self._sample_latent_from_prior(batch_x.shape[0])

        # Generate fake samples
        batch_fake = self.generator(z)

        # Calculate Generator loss for fake samples
        loss = -1 * self.discriminator(batch_fake).mean()

        # Do Backpropagation Manually
        self.manual_backward(loss)

        # Update Generator
        optim_G.step()

        # Log losses
        self.log("generator_loss", loss, on_step=True, on_epoch=True, prog_bar=True)

        return loss

    def on_train_start(self) -> None:
        """Check if the module has two optimizers."""
        assert (
            len(self.optimizers()) == 2  # type: ignore[arg-type]
        ), "WGAN-GP requires two optimizers, one for the generator and one for the discriminator"
        return

    def training_step(
        self, batch: torch.Tensor, batch_idx: int  # noqa: ARG002
    ) -> STEP_OUTPUT:
        """Training step for the WGAN-GP algorithm.

        Args:
            batch (torch.Tensor): Batch of real samples.
            batch_idx (int): Index of the current batch.

        Returns:
            STEP_OUTPUT: Dictionary containing the loss.
        """
        # Resolve batch
        batch = self._resolve_batch(batch)

        # Do Discriminator Step for n_critic times
        for _ in range(self.critic_iter):
            self._wgan_discriminator_step(batch)

        # Do Generator Step
        loss = self._wgan_generator_step(batch)

        return {"loss": loss}

    def configure_optimizers(self) -> list:
        """Configure optimizers for training.

        Returns:
            list: List of optimizers for the generator and discriminator.
        """
        optim_G = torch.optim.AdamW(
            self.generator.parameters(), lr=1e-4, betas=(0.0, 0.9)
        )
        optim_D = torch.optim.AdamW(
            self.discriminator.parameters(), lr=1e-4, betas=(0.0, 0.9)
        )
        return [optim_G, optim_D]
